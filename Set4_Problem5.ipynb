{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os.path import join\n",
    "\n",
    "# Assumes there is a .obj directory in the notebook folder\n",
    "def save_obj(obj, name, root=''):\n",
    "    with open(join(root, '.obj', name + '.pkl'), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name, root=''):\n",
    "    with open(join(root, '.obj/', name + '.pkl'), 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the CIFAR10 dataset. It has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training an image classifier\n",
    "----------------------------\n",
    "\n",
    "We will do the following steps in order:\n",
    "\n",
    "1. Load and normalizing the CIFAR10 training and test datasets using\n",
    "   ``torchvision``\n",
    "2. Define a Convolution Neural Network\n",
    "3. Define a loss function\n",
    "4. Train the network on the training data\n",
    "5. Test the network on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _num_flat_features(x):\n",
    "    size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "    num_features = 1\n",
    "    for s in size:\n",
    "        num_features *= s\n",
    "    return num_features\n",
    "\n",
    "\n",
    "class Net_C(nn.Module):\n",
    "    def __init__(self, M, p, N):\n",
    "        '''\n",
    "        M is the number of output channels, p is the convolution kernel size, \n",
    "        N is the max pooling kernel (ideally, it is a divisor of 33-p)\n",
    "        '''\n",
    "        super(Net_C, self).__init__()\n",
    "        # 3 input image channel, M output channels, pxpx3 square convolution; bias=True is default\n",
    "        self.conv1 = nn.Conv2d(3, M, p)\n",
    "        self.pool1 = nn.MaxPool2d(N)\n",
    "        self.fc1 = nn.Linear(M*((33-p)//N)**2, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = x.view(-1, self.num_flat_features(x)) # 32 * 32 * 3\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        return _num_flat_features(x)\n",
    "    \n",
    "\n",
    "class Net_B(nn.Module):\n",
    "    def __init__(self, M):\n",
    "        super(Net_B, self).__init__()\n",
    "        self.fc1 = nn.Linear(32 * 32 * 3, M)\n",
    "        self.fc2 = nn.Linear(M, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.num_flat_features(x)) # 32 * 32 * 3\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        return _num_flat_features(x)\n",
    "    \n",
    "\n",
    "class Net_A(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_A, self).__init__()\n",
    "        self.fc1 = nn.Linear(32 * 32 * 3, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.num_flat_features(x)) # 32 * 32 * 3\n",
    "        x = self.fc1(x) \n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        return _num_flat_features(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net, iter_trainloader, criterion, optimizer, epochs):\n",
    "    '''\n",
    "    Trains net and returns epoch-wise test loss\n",
    "    '''\n",
    "    epochs_train_loss = []\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        size = 0\n",
    "        for data in iter_trainloader: # AG: What is happening with this 0? \n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # sum statistics\n",
    "            size += 1\n",
    "            running_loss += loss.item()\n",
    "        # record loss    \n",
    "        epochs_train_loss.append(running_loss / size)\n",
    "    return epochs_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_net(net, iter_testloader, criterion):\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        test_loss = 0.0\n",
    "        size = 0\n",
    "        for data in iter_testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # Sum statistics\n",
    "            test_loss += loss.item()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            size += 1\n",
    "    return test_loss / size, 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_accuracy(net, iter_testloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared criterion for loss\n",
    "criterion_all = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure everything works...\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_A(\n",
      "  (fc1): Linear(in_features=3072, out_features=10, bias=True)\n",
      ")\n",
      "Net_B(\n",
      "  (fc1): Linear(in_features=3072, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "Net_C(\n",
      "  (conv1): Conv2d(3, 100, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=19600, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create new NN's, print\n",
    "# netA = Net_A()\n",
    "# netB = Net_B(100)\n",
    "# netC = Net_C(100, 5, 2)\n",
    "print(netA)\n",
    "print(netB)\n",
    "print(netC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6351,  0.1992,  0.3159,  0.0204,  0.0754, -0.0463,  0.3125, -0.0823,\n",
      "         -0.0979, -0.3068],\n",
      "        [-0.0114,  0.1704,  0.0322, -0.2838,  0.1890, -0.3320,  0.0448, -0.2956,\n",
      "         -0.2089, -0.3648],\n",
      "        [ 0.1855,  0.4247,  0.1155, -0.0184, -0.1142, -0.2607,  0.3533, -0.4119,\n",
      "         -0.4639,  0.0784],\n",
      "        [-0.1018,  0.0246,  0.2024,  0.1081,  0.0579, -0.4817,  0.1117, -0.4587,\n",
      "         -0.0535,  0.0337]], grad_fn=<ThAddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# This code made sure the NN's functioned correctly\n",
    "test_input = torch.randn(4, 3, 32, 32)\n",
    "for inet in [netB]:\n",
    "    out = inet(test_input)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "working with Network A\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netA = Net_A()\n",
    "resA=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "netA(test_input);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare some optimizer for Net A\n",
    "optimizerA = optim.SGD(netA.parameters(), lr=0.05, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\t 79.920, 76.249, 73.668, 71.825, 70.093, 69.397\n",
      "test:\t 76.059\n",
      "28% accuracy on 10000 test images.\n",
      "CPU times: user 3min 1s, sys: 8.29 s, total: 3min 10s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "resA.extend(train_net(netA, trainloader, criterion_all, optimizerA, 2))\n",
    "print('train:\\t', ', '.join(['%.3f' % i for i in resA]))\n",
    "loss, percent = test_net(netA, testloader, criterion_all)\n",
    "print('test:\\t', '%.3f' % loss )\n",
    "print('%d%% accuracy on 10000 test images.' % percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "3 miuntes for 2 epochs => 15 minutes for 10 epochs\n",
    "\n",
    "If 3 hours alloted, number of random guesses allowed: 4*3 = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "working with Network B\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "netB = Net_B(400)\n",
    "resB=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerB = optim.SGD(netB.parameters(), lr=0.01, momentum=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4090, -0.0641,  0.0474, -0.1546,  0.2635,  0.1513,  0.3057, -0.3078,\n",
       "         -0.2881,  0.1360],\n",
       "        [-0.1789, -0.2995, -0.0761,  0.1897,  0.1121,  0.3744,  0.0944, -0.1846,\n",
       "         -0.1328, -0.1426],\n",
       "        [-0.0523, -0.0421, -0.3660,  0.0535, -0.1048,  0.1844, -0.0085, -0.0769,\n",
       "         -0.2485, -0.0773],\n",
       "        [-0.0363, -0.0456,  0.4121, -0.3898, -0.1444,  0.0878,  0.1636, -0.3042,\n",
       "         -0.1371,  0.0013]], grad_fn=<ThAddmmBackward>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netB(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\t 2.036, 2.048\n",
      "test:\t 2.109\n",
      "38% accuracy on 10000 test images.\n",
      "CPU times: user 12min 4s, sys: 14.7 s, total: 12min 18s\n",
      "Wall time: 5min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "resB.extend(train_net(netB, trainloader, criterion_all, optimizerB, 2))\n",
    "print('train:\\t', ', '.join(['%.3f' % i for i in resB]))\n",
    "loss, percent = test_net(netB, testloader, criterion_all)\n",
    "print('test:\\t', '%.3f' % loss )\n",
    "print('%d%% accuracy on 10000 test images.' % percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "m=100,lr=.01,p=[0.5,.7]\n",
    "train:\t 1.916, 1.851\n",
    "test:\t 1.988\n",
    "37% accuracy on 10000 test images.\n",
    "CPU times: user 4min 57s, sys: 11.4 s, total: 5min 8s\n",
    "Wall time: 2min 23s\n",
    "\n",
    "Why did B report nan for solutions with lr = 0.05 and p = .9? That's garbage.\n",
    "Tried .1. Still garbage. So can't learn too quickly.\n",
    "I tried  with lr = .01 and p=0.5 and it converged nicely.\n",
    "I tried it with lr=0.001, momentum=0.5 and I got answers but it never moved.\n",
    "\n",
    "Similar time for A and B: B was 2min 21s for lr = .01 and p=.5 and M = 100\n",
    "When M=400, this time went to: 5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "working with Network C\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "netC = Net_C(100, 6, 4)\n",
    "resC=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "netC(test_input);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerC = optim.SGD(netC.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "resC.extend(train_net(netC, trainloader, criterion_all, optimizerC, 2))\n",
    "print('train:\\t', ', '.join(['%.3f' % i for i in resC]))\n",
    "loss, percent = test_net(netC, testloader, criterion_all)\n",
    "print('test:\\t', '%.3f' % loss )\n",
    "print('%d%% accuracy on 10000 test images.' % percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(name='netC_lr01_m5_100_6_4', obj=[netC, resC])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "Training lr=0.01, p=.5 on Net_C(100, 5, 2) gives time of 6 minutes. Errors were at ~1.2 at 2 epochs. \n",
    "\n",
    "netC = Net_C(100, 6, 4)\n",
    "lr=0.01, momentum=0.5\n",
    "train:\t 1.392, 1.223\n",
    "test:\t 1.404\n",
    "55% accuracy on 10000 test images.\n",
    "CPU times: user 11min 35s, sys: 13.5 s, total: 11min 48s\n",
    "Wall time: 5min 5s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fun novelty functions \n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_percent_error(net, iter_testloader):\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(4):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "\n",
    "    for i in range(10):\n",
    "        print('Accuracy of %5s : %2d %%' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane : 64 %\n",
      "Accuracy of   car : 71 %\n",
      "Accuracy of  bird : 43 %\n",
      "Accuracy of   cat : 33 %\n",
      "Accuracy of  deer : 50 %\n",
      "Accuracy of   dog : 44 %\n",
      "Accuracy of  frog : 75 %\n",
      "Accuracy of horse : 63 %\n",
      "Accuracy of  ship : 75 %\n",
      "Accuracy of truck : 55 %\n"
     ]
    }
   ],
   "source": [
    "categorical_percent_error(netC, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane : 58 %\n",
      "Accuracy of   car : 56 %\n",
      "Accuracy of  bird : 52 %\n",
      "Accuracy of   cat : 38 %\n",
      "Accuracy of  deer : 44 %\n",
      "Accuracy of   dog : 35 %\n",
      "Accuracy of  frog : 88 %\n",
      "Accuracy of horse : 57 %\n",
      "Accuracy of  ship : 56 %\n",
      "Accuracy of truck : 68 %\n"
     ]
    }
   ],
   "source": [
    "categorical_percent_error(netC, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
